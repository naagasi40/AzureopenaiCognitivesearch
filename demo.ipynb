{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing Necessary Azure SDK Clients for Cognitive Search and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This program utilizes Azure Cognitive Search and Azure OpenAI to answer questions\n",
    "from uploaded PDF documents in Azure Blob Storage.\n",
    "It leverages Python virtual environment for development.\n",
    "\"\"\"\n",
    "import os\n",
    "import openai\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import QueryType\n",
    "from azure.search.documents.models import QueryLanguage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Replace these with your own values, either in environment variables or directly here\n",
    "AZURE_STORAGE_ACCOUNT = os.environ.get(\"AZURE_STORAGE_ACCOUNT\") or \"your-storage-account-name\"\n",
    "AZURE_STORAGE_CONTAINER = os.environ.get(\"AZURE_STORAGE_CONTAINER\") or \"your-container-name\"\n",
    "AZURE_SEARCH_SERVICE = os.environ.get(\"AZURE_SEARCH_SERVICE\") or \"your-cg-search-service-name\"\n",
    "AZURE_SEARCH_INDEX = os.environ.get(\"AZURE_SEARCH_INDEX\") or \"your-cg-search-index-name\"\n",
    "AZURE_OPENAI_SERVICE = os.environ.get(\"AZURE_OPENAI_SERVICE\") or \"your-openai-service-name\"\n",
    "AZURE_OPENAI_GPT_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_GPT_DEPLOYMENT\") or \"your-model-name\"\n",
    "AZURE_OPENAI_CHATGPT_DEPLOYMENT = os.environ.get(\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\") or \"chat\"\n",
    "\n",
    "KB_FIELDS_CONTENT = os.environ.get(\"KB_FIELDS_CONTENT\") or \"content\"\n",
    "KB_FIELDS_CATEGORY = os.environ.get(\"KB_FIELDS_CATEGORY\") or \"category\"\n",
    "KB_FIELDS_SOURCEPAGE = os.environ.get(\"KB_FIELDS_SOURCEPAGE\") or \"metadata_storage_name\"\n",
    "\n",
    "# Used by the OpenAI SDK\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = f\"https://{AZURE_OPENAI_SERVICE}.openai.azure.com\"\n",
    "openai.api_version = \"2022-12-01\"\n",
    "\n",
    "# set your API key in the OPENAI_API_KEY environment variable instead\n",
    "openai.api_key = 'YOUR OPENAI API KEY'\n",
    "# az search admin-key show --resource-group <myresourcegroup> --service-name <myservice>\n",
    "search_key = \"YOUR Cognitive Search Admin/Query KEY\"\n",
    "\n",
    "# Use the current user identity to authenticate with Azure OpenAI, Cognitive Search and Blob Storage\n",
    "# (no secrets needed, \n",
    "# just use 'az login' locally, and managed identity when deployed on Azure).\n",
    "# If you need to use keys, use separate AzureKeyCredential instances with the \n",
    "# keys for each service\n",
    "# If you encounter a blocking error during a DefaultAzureCredntial resolution, you can exclude the problematic credential by using a parameter (ex. exclude_shared_token_cache_credential=True)\n",
    "az_credential = DefaultAzureCredential()\n",
    "azure_credential = AzureKeyCredential(search_key)\n",
    "# Set up clients for Cognitive Search and Storage\n",
    "search_client = SearchClient(\n",
    "    endpoint=f\"https://{AZURE_SEARCH_SERVICE}.search.windows.net\",\n",
    "    index_name=AZURE_SEARCH_INDEX,\n",
    "    credential=azure_credential)\n",
    "#Set up clients for Blob STORAGE\n",
    "blob_client = BlobServiceClient(\n",
    "    account_url=f\"https://{AZURE_STORAGE_ACCOUNT}.blob.core.windows.net\", \n",
    "    credential=az_credential)\n",
    "blob_container = blob_client.get_container_client(AZURE_STORAGE_CONTAINER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Prompt Template for querying into respective enterprise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \\\n",
    "\"You are an intelligent assistant helping users with their Diabetes related questions. \" + \\\n",
    "\"Use 'you' to refer to the individual asking the questions even if they ask with 'I'. \" + \\\n",
    "\"Answer the following question using only the data provided in the sources below. \" + \\\n",
    "\"\"\"\n",
    "\n",
    "###\n",
    "Question: 'What is Type 1 Diabetes?'\n",
    "\n",
    "Sources:\n",
    "info1.txt: Type 1 diabetes, also known as insulin-dependent diabetes or juvenile-onset diabetes, typically develops in childhood or adolescence. \n",
    "info2.pdf: Type 1 diabetes occurs when the immune system mistakenly attacks and destroys the insulin-producing beta cells in the pancreas.\n",
    "\n",
    "Answer:\n",
    "Type 1 diabetes, also known as insulin-dependent diabetes or juvenile-onset diabetes, typically develops in childhood or adolescence. It occurs when the immune system mistakenly attacks and destroys the insulin-producing beta cells in the pancreas. The exact cause of this autoimmune response is not fully understood, but genetic and environmental factors are thought to play a role.\n",
    "\n",
    "###\n",
    "Question: '{q}'?\n",
    "\n",
    "Sources:\n",
    "{retrieved}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching results from Cognitive Search based on user query. Results are fetched via text search. Semantic search is not applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What is type1 Diabetes?\"\n",
    "\n",
    "# Exclude category, to simulate scenarios where there's a set of docs you can't see\n",
    "exclude_category = None\n",
    "search = user_input\n",
    "print(\"Searching:\", search)\n",
    "print(\"-------------------\")\n",
    "# Filter out documents with a specific category\n",
    "filter = \"category ne '{}'\".format(exclude_category.replace(\"'\", \"''\")) if exclude_category else None\n",
    "# Perform the search using Azure Cognitive Search\n",
    "r = search_client.search(search,\n",
    "                         query_type=QueryType.SIMPLE,                          \n",
    "                         top=3)\n",
    "# Extract the relevant information from the search results\n",
    "results = [doc[KB_FIELDS_SOURCEPAGE] + \": \" + doc[KB_FIELDS_CONTENT].replace(\"\\n\", \"\").replace(\"\\r\", \"\") for doc in r]\n",
    "content = \"\\n\".join(results)\n",
    "print(\"***********************\")\n",
    "print(content)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Text Summarization function. If document size is large and text is complex, pre trained models \n",
    "like peagus/BART from transformers (HuggingFace) can be leveraged. \n",
    "\n",
    "Note: Execution time of models might vary on the token length provided and which in turn would be dependent on text provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "\n",
    "def summarize(text, per):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    word_frequencies = {}\n",
    "\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "\n",
    "    # Maximum frequency of word\n",
    "    max_frequency = max(word_frequencies.values())\n",
    "    # Normalization of word frequency\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = word_frequencies[word]/max_frequency\n",
    "    # In this part, each sentence is weighed based on how often it contains the token.\n",
    "    sentence_tokens = [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
    "    select_length = int(len(sentence_tokens)*per)\n",
    "    # Summary for the sentences with maximum score. Here, each sentence in the list is of spacy.span type\n",
    "    summary = nlargest(select_length, sentence_scores, key=sentence_scores.get)\n",
    "    # Prepare for final summary\n",
    "    final_summary = [word.text for word in summary]\n",
    "    #convert to a string\n",
    "    summary = ''.join(final_summary)\n",
    "    return summary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Parameters for text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Cleaning Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "\n",
    "import re\n",
    "def text_cleaner(text):\n",
    "    newString = text.lower()\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    tokens = [w for w in newString.split() if not w in STOP_WORDS]\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>=3:                  #removing short word\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution of text summarization and then post processing of summarized text via Text Cleaning before passing to GPT Prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarize(content, 0.05)\n",
    "result = text_cleaner(summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have text summarized, that means token length would be significantly reduced and even cheaper OpenAI models like DaVinci instead of GPT 3.5 turbo or GPT4 can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the prompt for the OpenAI model using the template and retrieved information\n",
    "prompt = template.format(q=user_input, retrieved=result)\n",
    "# Call the OpenAI GPT model to get the answer\n",
    "completion = openai.Completion.create(\n",
    "    engine= AZURE_OPENAI_GPT_DEPLOYMENT, \n",
    "    prompt=prompt, \n",
    "    temperature= 0.3, \n",
    "    max_tokens=1024, \n",
    "    n=1, \n",
    "    stop=[\"\\n\"])\n",
    "# Print the answer and additional information\n",
    "print(completion)\n",
    "print({\"data_points\": results, \"answer\": completion.choices[0].text, \"thoughts\": f\"Question:<br>{user_input}<br><br>Prompt:<br>\" + prompt.replace('\\n', '<br>')})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: Using google's Peagus model from Hugging Face.\n",
    "Note: Model has a constrained of 1024 input tokens and processing text summarization of longer texts will result in higher execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusForConditionalGeneration\n",
    "from transformers import PegasusTokenizer\n",
    "# Pick model\n",
    "model_name = \"google/pegasus-xsum\"\n",
    "\n",
    "# Load pretrained tokenizer\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to model's token limit, we are dividing our text in batches and passing to model.\n",
    "Later, we are extracting summary from each batch and joining them together.\n",
    "\n",
    "Note: this method may not be very efficient while computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text and split it into smaller sections\n",
    "section_size = 1024  # Adjust this value based on the model's token limit\n",
    "\n",
    "# Passing my Cognitive search 'content':\n",
    "sections = [content[i:i+section_size]\n",
    "            for i in range(0, len(content), section_size)]\n",
    "\n",
    "# Initialize the list to store summaries\n",
    "summaries = []\n",
    "\n",
    "# Generate summaries for each section\n",
    "for section in sections:\n",
    "    # Tokenize the section\n",
    "    input_ids = pegasus_tokenizer.encode(\n",
    "        section, truncation=True, max_length=1024, return_tensors='pt')\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = pegasus_model.generate(\n",
    "        input_ids, num_beams=4, max_length=100, early_stopping=True)\n",
    "\n",
    "    # Decode the summary tokens back to text\n",
    "    summary = pegasus_tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "\n",
    "    # Append the summary to the list\n",
    "    summaries.append(summary)\n",
    "\n",
    "# Combine the summaries into a final summary\n",
    "final_summary = \" \".join(summaries)\n",
    "\n",
    "# Print the final summary\n",
    "print(final_summary)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
